# Sprint 1 Planning - Weeks 1-2

## Sprint 1 - Weeks 1-2 Activities Overview

![Project Deliverables Delivery Timeline](https://res.cloudinary.com/dyhjjms8y/image/upload/v1760833550/ioz1nqma2ovf6bgqnqi4.png)

| Atividades | Data | Prof | Eixo | Unidade |
|---|---|---|---|---|
| Sprint Planning 1 | 15/10/2025 | Hermano | - | Sprint Planning |
| Learning the Dataset with Data Model Canvas* | 16/10/2025 | Thais Rodrigues Ne... | COM | Semana 01 |
| Workshop | 17/10/2025 | Hermano | - | Workshop |
| Advanced Exploratory Data Analysis | 20/10/2025 | Afonso Cesar Lelis ... | COM | Semana 02 |
| Unraveling the Data: Preprocessing Operations* | 21/10/2025 | Henrique Mohalle... | MTF | Semana 02 |
| Best Practices for UX in Big Data Visualization* | 22/10/2025 | Julia Stateri | UEX | Semana 02 |
| Architectural Tactics for Data Flow and Transformations | 23/10/2025 | Afonso Cesar Lelis ... | COM | Semana 02 |
| Sprint Review and Retrospective 1 | 24/10/2025 | - | - | Sprint Review |

**Legenda:** * = Aulas com atividades ponderadas (graded activities)

## Sprint Overview
**Duration**: 2 weeks (10 working days)  
**Development Time**: 2 hours per day per student  
**Total Development Hours per Student**: 20 hours  
**Team Size**: Variable (adjust tasks accordingly)

## Deliverables and Weight Distribution (Sprint 1 - Week 02)

| Deliverable | Weight | Percentage | Description | Estimated Hours |
|-------------|--------|------------|-------------|-----------------|
| **Data Discovery with Data Model Canvas** | 3 | 21.4% | Complete data model canvas with value proposition, data structure, user profile, and comprehensive analysis | 4.3 hours |
| **Market and Risk Analysis Workbook** | 3 | 21.4% | Value proposition canvas, TAM/SAM/SOM analysis, and risk matrix with mitigation strategies | 4.3 hours |
| **Architecture Activity** | 3 | 21.4% | Data architecture proposal covering ingestion, processing, and visualization with scalability focus | 4.3 hours |
| **Data Narrative Oriented to the DMC** | 2 | 14.3% | Data-driven narrative with consumer persona and storytelling aligned with DMC | 2.9 hours |
| **Six Ways Big Data Can Improve Customer Experience** | 2 | 14.3% | In-class graded activity on big data applications | 2.9 hours |
| **Data Preprocessing** | 1 | 7.1% | Graded activity on data preprocessing techniques | 1.4 hours |

**Total Weight**: 14 points  
**Total Estimated Hours**: 20 hours per student

## Task Division Instructions for ChatGPT

### Step 1: Prepare Your Team Information
Before using ChatGPT, gather the following information:
- **Number of team members**
- **Each member's experience level** (Beginner/Intermediate/Advanced)
- **Preferred technologies** (if any)
- **Available time slots** for each member

### Step 2: Use the ChatGPT Prompt Template
Copy and paste the prompt below into ChatGPT, filling in your team's specific information:

---

## ChatGPT Prompt Template

```
I need help dividing tasks for a 2-week data engineering sprint. Here are the details:

**TEAM INFORMATION:**
- Number of team members: [INSERT NUMBER]
- Team member experience levels: [INSERT LEVELS - e.g., 2 beginners, 1 intermediate, 1 advanced]
- Available development time: 2 hours per day per person for 10 days (20 hours total per person)

**SPRINT DELIVERABLES (with weights):**
1. Data Discovery with Data Model Canvas (21.4% - 4.3 hours per person)
2. Market and Risk Analysis Workbook (21.4% - 4.3 hours per person)
3. Architecture Activity (21.4% - 4.3 hours per person)
4. Data Narrative Oriented to the DMC (14.3% - 2.9 hours per person)
5. Six Ways Big Data Can Improve Customer Experience (14.3% - 2.9 hours per person)
6. Data Preprocessing (7.1% - 1.4 hours per person)

**REQUIREMENTS:**
- Each deliverable must be completed by the end of the sprint
- Tasks should be divided considering experience levels
- Include setup and learning time for new technologies
- Consider dependencies between tasks
- Include code review and documentation time
- Follow GitFlow workflow with feature branches
- Use Conventional Commits for all changes

**TECHNOLOGIES TO USE:**
- Python with Poetry for dependency management
- Jupyter Notebooks for data analysis
- Cloud platform (AWS/Azure/GCP) for data lake
- UML tools for architecture diagrams
- Git for version control

Please create a detailed task breakdown with:
1. Task assignments for each team member
2. Daily schedule for 2 weeks
3. Dependencies and critical path
4. Risk mitigation strategies
5. Quality checkpoints and review sessions
```

---

## Example Task Division (4-person team)

### Team Composition Example:
- **Alice** (Advanced): Team Lead, Architecture
- **Bob** (Intermediate): Data Analysis Specialist  
- **Carol** (Intermediate): Data Engineering
- **David** (Beginner): Documentation and Support

### Week 1 Schedule:

| Day | Alice (Advanced) | Bob (Intermediate) | Carol (Intermediate) | David (Beginner) |
|-----|------------------|-------------------|---------------------|------------------|
| **Day 1** | Project setup, Data Model Canvas planning | Environment setup, data exploration | Environment setup, architecture research | Environment setup, documentation setup |
| **Day 2** | Data Model Canvas - Value Proposition | Market analysis - TAM/SAM/SOM research | Architecture design - data flow planning | Research data architecture concepts |
| **Day 3** | Data Model Canvas - Data Structure | Risk matrix creation, mitigation strategies | Architecture implementation start | Documentation of architecture decisions |
| **Day 4** | Data Model Canvas - User Profile | Market analysis finalization | Architecture testing and validation | Code review preparation, documentation |
| **Day 5** | Data Model Canvas - Use Cases | Risk analysis finalization | Architecture documentation | Documentation review and updates |

### Week 2 Schedule:

| Day | Alice (Advanced) | Bob (Intermediate) | Carol (Intermediate) | David (Beginner) |
|-----|------------------|-------------------|---------------------|------------------|
| **Day 6** | Data Model Canvas finalization | Market analysis review and validation | Architecture testing and optimization | Data preprocessing activity |
| **Day 7** | Data Narrative - Consumer Persona | Risk analysis review and updates | Architecture documentation completion | Big Data customer experience activity |
| **Day 8** | Data Narrative - Storytelling | Market analysis presentation prep | Architecture final review | Data preprocessing finalization |
| **Day 9** | Integration testing, final reviews | Risk analysis final validation | Architecture delivery preparation | All activities final review |
| **Day 10** | Final integration, delivery preparation | Final market analysis delivery | Final architecture delivery | Final documentation delivery |

## Critical Path and Dependencies

### Phase 1: Foundation (Days 1-3)
- Environment setup (all members)
- Data Model Canvas planning (Alice)
- Market analysis research (Bob)
- Architecture design (Carol)
- Documentation setup (David)

### Phase 2: Development (Days 4-7)
- Data Model Canvas implementation (Alice)
- Market and Risk analysis implementation (Bob)
- Architecture implementation (Carol)
- Data preprocessing and Big Data activities (David)

### Phase 3: Integration (Days 8-10)
- Data Narrative creation (Alice)
- Final market analysis validation (Bob)
- Architecture finalization (Carol)
- All activities final review (David)
- Delivery preparation (all)

## Quality Checkpoints

### Daily Standups (15 minutes)
- Progress review
- Blockers identification
- Next day planning

### Mid-Sprint Review (Day 5)
- Architecture review
- EDA progress assessment
- Data lake progress check
- Risk assessment

### Final Review (Day 9)
- Complete system testing
- Documentation review
- Delivery preparation
- Presentation rehearsal

## Risk Mitigation Strategies

### Technical Risks:
- **Cloud setup delays**: Start cloud account setup on Day 1
- **Data access issues**: Identify and request data access early
- **Integration problems**: Plan integration testing from Day 6

### Team Risks:
- **Knowledge gaps**: Pair programming sessions
- **Time management**: Daily progress tracking
- **Communication**: Daily standups and Slack channels

## Success Metrics

### Technical Metrics:
- [ ] All three deliverables completed
- [ ] Code follows clean code principles
- [ ] GitFlow workflow implemented
- [ ] Conventional commits used
- [ ] Documentation complete

### Team Metrics:
- [ ] All team members contribute equally
- [ ] No single points of failure
- [ ] Knowledge sharing sessions completed
- [ ] Code reviews conducted

## Next Steps

1. **Immediate Actions**:
   - Use the ChatGPT prompt with your team's information
   - Customize the task division based on your team size
   - Set up communication channels (Slack/Discord)
   - Create shared project repository

2. **Before Starting Development**:
   - Confirm cloud platform access
   - Set up development environments
   - Establish Git workflow
   - Plan daily standup times

3. **During Development**:
   - Follow the daily schedule
   - Conduct regular code reviews
   - Update documentation continuously
   - Track progress against milestones

Remember: This is a collaborative effort. Use ChatGPT to customize the plan for your specific team, but always validate the suggestions with your team members and adjust based on your actual progress and challenges.
